---
title: "A Stochastic Polyhedral Approximation Method for Decentralized Composite Bilevel Optimization"
collection: publications
permalink: /publication/2023-SPAM
excerpt: 'In this paper, we propose a Stochastic Polyhedral Approximation Method (SPAM) for composite decentralized 
bi-level optimization problems.'
date: 2023-05-21
venue: 'NeurIPS (in submission)'
date_label: 'Preprint'

[//]: # (paperurl: 'https://openreview.net/pdf?id=uqyanADT9y')
citation: 
---
## Abstract

Author: Ya Liu, Kai Yang*, **Haibo Zhao**, Yu Zhu, Keying Yang

Bilevel optimization has received increasing attention in many machine learning problems, e.g., meta-learning, 
reinforcement learning, and hyperparameter optimization. Existing studies on bilevel optimization are primarily 
geared toward centralized and smooth settings. In practice, the data and learning tasks may be lo    cated at 
different computing nodes. Besides, nonsmooth terms in objectives arise naturally in machine learning applications that involve regularization or penalty. In this paper, we propose a Stochastic Polyhedral Approximation Method (SPAM) for composite decentralized bilevel optimization problems. The proposed SPAM allows networked agents to solve bilevel optimization problems that both upper-level and lower-level objective functions are nonconvex and contain non-smooth terms in a fully decentralized manner. We also establish that the proposed SPAM can achieve an convergence rate. Numerical experiments on public datasets corroborate the effectiveness and efficiency of the proposed algorithm.

## My Contribution
- Completed the mathematical derivation of the algorithmic framework incorporating the proximal gradient.
- Implemented the algorithm for meta-learning and hyperparameter optimization using the PyTorch framework.
- Reproduced the code from similar papers and conducted a comparison with our method.

[//]: # ([Download paper here]&#40;https://openreview.net/pdf?id=uqyanADT9y&#41;)

